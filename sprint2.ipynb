{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de8e0dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (1.35.44)\n",
      "Requirement already satisfied: nibabel in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (5.3.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: torch in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (2.5.1+cu124)\n",
      "Requirement already satisfied: torchvision in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (0.20.1+cu124)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (2.5.1+cu124)\n",
      "Requirement already satisfied: pillow in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (11.0.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.44 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from boto3) (1.35.44)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from boto3) (0.10.3)\n",
      "Requirement already satisfied: packaging>=20 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from nibabel) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from nibabel) (4.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: filelock in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from torch) (75.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (5.28.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.67.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from botocore<1.36.0,>=1.35.44->boto3) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.1.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install the required packages for the notebook\n",
    "\n",
    "%pip install boto3 nibabel numpy matplotlib torch torchvision torchaudio pillow tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1950f2bf-d1fd-4c35-b6b7-8318c816c6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprint 1: Reading NII files from S3 and saving PNGs\n",
    "\n",
    "import boto3\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import io\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Initialize S3 resource and specify bucket and folder details\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'chemocraft-data'\n",
    "folder_path = 'MICCAI_BraTS2020_TrainingData/'\n",
    "# folder_path = 'Data/BraTS20_Training_369 copy/'\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "def plot_slice(data, crop, slice_idx, filename):\n",
    "    # Crop the specified slice\n",
    "    slice_2d = data[:, :, slice_idx]\n",
    "    cropped_slice = slice_2d[crop[0][0]:crop[0][1], crop[1][0]:crop[1][1]]\n",
    "    \n",
    "    # Display the cropped slice with matplotlib\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(cropped_slice, cmap='gray')\n",
    "    plt.title(f'Slice {slice_idx} of {filename}')\n",
    "    plt.axis('off')  # Hide axes for cleaner display\n",
    "    plt.show()\n",
    "\n",
    "def savePNG(data, crop, filename):    \n",
    "    # Prepare directory structure\n",
    "    fileWOext = filename.split(\".\")[0]\n",
    "    TrainingCount = fileWOext.split(\"_\")[-2]\n",
    "    ScanType = fileWOext.split(\"_\")[-1]\n",
    "    slice_path = f\"brain_slices/{TrainingCount}/{ScanType}/\"\n",
    "    print(f\"Saving in directory: {slice_path}\")\n",
    "\n",
    "    # Iterate through each slice in the Z-Dimiension data and save as PNG\n",
    "    for slice_idx in range(data.shape[2]):\n",
    "        # Crop each slice\n",
    "        slice_2d = data[:, :, slice_idx]\n",
    "        cropped_slice = slice_2d[crop[0][0]:crop[0][1], crop[1][0]:crop[1][1]]\n",
    "        png_filename = f\"{slice_path}{slice_idx}.png\"\n",
    "        \n",
    "        # Local Saving\n",
    "        # try:\n",
    "        #     # Create directories as needed and save each slice\n",
    "        #     os.makedirs(slice_path, exist_ok=True)\n",
    "        #     mpimg.imsave(png_filename, cropped_slice, cmap='gray')\n",
    "        # except Exception as e:\n",
    "        #     print(f\"ERROR: directory could not be made due to {e}\")\n",
    "        \n",
    "        # Upload each PNG to S3\n",
    "        with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as temp_png:\n",
    "            mpimg.imsave(temp_png.name, cropped_slice, cmap='gray')\n",
    "            temp_png.flush()\n",
    "            temp_png.seek(0)\n",
    "            temp_png_name = temp_png.name  # Store the name to use it after the file is closed\n",
    "\n",
    "        try:\n",
    "            s3.Bucket(bucket_name).upload_file(temp_png_name, f\"Akshay/{png_filename}\")\n",
    "            os.remove(temp_png_name)\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Could not upload or delete temporary PNG file due to {e}\")\n",
    "\n",
    "def render_nii_from_s3(filename, path):\n",
    "    print(f\"Fetching file: {filename}\")\n",
    "\n",
    "    try:\n",
    "        obj = bucket.Object(path + filename)\n",
    "        file_stream = io.BytesIO(obj.get()['Body'].read())\n",
    "    except s3.meta.client.exceptions.NoSuchKey as e:\n",
    "        print(f\"ERROR: The specified key does not exist: {path + filename}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: An unexpected error occurred: {e}\")\n",
    "        return\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(suffix='.nii', delete=False) as temp_file:  # Prevent auto-deletion\n",
    "        temp_file.write(file_stream.getvalue())\n",
    "        temp_file.flush()\n",
    "\n",
    "        temp_file_path = temp_file.name\n",
    "        print(f\"Temporary file created: {temp_file_path}\")\n",
    "\n",
    "    try:\n",
    "        img = nib.load(temp_file_path)\n",
    "        data = img.get_fdata()\n",
    "\n",
    "        print(f\"Data shape for {filename}: {data.shape}\")\n",
    "        \n",
    "        if data.size == 0:\n",
    "            print(f\"No data found in {filename}\")\n",
    "            return\n",
    "\n",
    "        # Define crop dimensions\n",
    "        cropleft = 25\n",
    "        cropright = data.shape[0] - 15\n",
    "        cropbottom = data.shape[1] - 40\n",
    "        croptop = 40\n",
    "        \n",
    "        crop = np.array([[croptop, cropbottom], [cropleft, cropright]])\n",
    "        \n",
    "        # Save the PNGs and plot a sample slice\n",
    "        savePNG(data, crop, filename)\n",
    "        \n",
    "        # slice_idx = 88  # Choose a slice index for sample display\n",
    "        # plot_slice(data, crop, slice_idx, filename)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file {filename}: {e}\")\n",
    "        \n",
    "    finally:\n",
    "        try:\n",
    "            os.remove(temp_file_path)\n",
    "            print(f\"Deleted temporary file: {temp_file_path}\")\n",
    "        except OSError as cleanup_error:\n",
    "            print(f\"Error deleting temp file: {cleanup_error}\")\n",
    "\n",
    "def find_and_render_nii_files():\n",
    "    found_files = False\n",
    "\n",
    "    subfolders = set()  # use a set to ensure unique subfolder names\n",
    "    for obj in bucket.objects.filter(Prefix=folder_path):\n",
    "        # Get the path after the 'Data/' prefix and split it by '/'\n",
    "        path_parts = obj.key[len(folder_path):].split('/')\n",
    "        \n",
    "        # Check if there's at least one part (indicating a subfolder)\n",
    "        if len(path_parts) > 1:\n",
    "            subfolders.add(f'{path_parts[0]}/')  # Add the subfolder name\n",
    "            \n",
    "    subfolders = sorted(subfolders)\n",
    "\n",
    "    print(f\"Root Directory: {folder_path.split('/')[0]}\")\n",
    "    # print(subfolders)\n",
    "\n",
    "    for subfolder in subfolders:\n",
    "        path = folder_path + subfolder\n",
    "        print(f\"Reading S3 in {path}\")\n",
    "        for obj in bucket.objects.filter(Prefix=path):\n",
    "            if obj.key.endswith('.nii'):\n",
    "                print(f\"path: {path}\")\n",
    "                found_files = True\n",
    "                filename = obj.key.split('/')[-1]  # Extract filename from path\n",
    "                print(f\"Found .nii file: {filename}\")\n",
    "                render_nii_from_s3(filename, path)\n",
    "\n",
    "    if not found_files:\n",
    "        print(f\"No .nii files found in the folder {folder_path}\")\n",
    "\n",
    "# Main function\n",
    "# find_and_render_nii_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "869e03d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Generator Model\n",
      "Output shape: (200, 160, 1)\n",
      "Building Discriminator Model\n",
      "Input shape: (200, 160, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\grnpr\\anaconda3\\envs\\chemocraft\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Users\\grnpr\\anaconda3\\envs\\chemocraft\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Generator\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Generator\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64000</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">6,464,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,792</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,616</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_2              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64000\u001b[0m)          │     \u001b[38;5;34m6,464,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │       \u001b[38;5;34m147,584\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │        \u001b[38;5;34m73,792\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m8\u001b[0m)     │         \u001b[38;5;34m4,616\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_2              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │           \u001b[38;5;34m129\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,690,121</span> (25.52 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,690,121\u001b[0m (25.52 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,690,121</span> (25.52 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,690,121\u001b[0m (25.52 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Discriminator\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Discriminator\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16640</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,641</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │           \u001b[38;5;34m544\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │        \u001b[38;5;34m16,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu (\u001b[38;5;33mLeakyReLU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m32,832\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_1 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │       \u001b[38;5;34m131,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_2 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16640\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │        \u001b[38;5;34m16,641\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">197,633</span> (772.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m197,633\u001b[0m (772.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">197,633</span> (772.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m197,633\u001b[0m (772.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"GAN\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"GAN\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Generator (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">6,690,121</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Discriminator (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">197,633</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Generator (\u001b[38;5;33mSequential\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │     \u001b[38;5;34m6,690,121\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Discriminator (\u001b[38;5;33mSequential\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │       \u001b[38;5;34m197,633\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,887,754</span> (26.27 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,887,754\u001b[0m (26.27 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,690,121</span> (25.52 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,690,121\u001b[0m (25.52 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">197,633</span> (772.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m197,633\u001b[0m (772.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sprint 2: GAN for Brain MRI Generation\n",
    "\n",
    "import io\n",
    "from io import BytesIO\n",
    "import keras\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models # type: ignore\n",
    "from tensorflow.keras.optimizers import Adam # type: ignore\n",
    "from tensorflow.keras.utils import Sequence # type: ignore # type: ignore\n",
    "from keras.preprocessing.image import img_to_array, load_img # type: ignore\n",
    "\n",
    "# Initialize S3 resource and specify bucket and folder details\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'chemocraft-data'\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "folder_prefix = \"Akshay/brain_slices/\"\n",
    "\n",
    "def load_images_from_s3(bucket, folder_prefix):\n",
    "    print(f\"Loading images from S3 bucket: {bucket_name}/{folder_prefix}\")\n",
    "    images = []\n",
    "    for obj in bucket.objects.filter(Prefix=folder_prefix):\n",
    "        if obj.key.endswith('.png'):\n",
    "            file_stream = io.BytesIO(obj.get()['Body'].read())\n",
    "            image = load_img(file_stream, target_size=(200, 160), color_mode='grayscale')\n",
    "            image = img_to_array(image) / 255.0  # Normalize to [0, 1]\n",
    "            images.append(image)\n",
    "    return np.array(images)\n",
    "\n",
    "\n",
    "# Generator Model\n",
    "def build_generator(latent_dim=100, output_shape=(200, 160, 1)):\n",
    "    \"\"\"\n",
    "    Builds the generator model for a GAN.\n",
    "    \n",
    "    Parameters:\n",
    "    latent_dim (int): The size of the input latent vector.\n",
    "    output_shape (tuple): The desired shape of the generated images (height, width, channels).\n",
    "    \n",
    "    Returns:\n",
    "    A compiled Keras Sequential model.\n",
    "    \"\"\"\n",
    "    model = models.Sequential(name=\"Generator\")\n",
    "    print(\"Building Generator Model\")\n",
    "    \n",
    "    # Input layer\n",
    "    print(f\"Output shape: {output_shape}\")\n",
    "    model.add(layers.Input(shape=(latent_dim,)))\n",
    "    model.add(layers.Dense(128 * 25 * 20, activation=\"relu\"))\n",
    "    model.add(layers.Reshape((25, 20, 128)))\n",
    "    \n",
    "    # Transposed convolutional layers to upsample\n",
    "    model.add(layers.Conv2DTranspose(128, kernel_size=3, strides=2, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(layers.Conv2DTranspose(64, kernel_size=3, strides=2, padding=\"same\", activation=\"relu\"))\n",
    "    # model.add(layers.Conv2DTranspose(32, kernel_size=4, strides=2, padding=\"same\", activation=\"relu\"))  \n",
    "    # model.add(layers.Conv2DTranspose(8, kernel_size=4, strides=4, padding=\"same\", activation=\"relu\"))  \n",
    "    model.add(layers.Conv2D(8, kernel_size=3, padding=\"same\", activation=\"tanh\"))\n",
    "    model.add(layers.Conv2DTranspose(output_shape[2], kernel_size=4, strides=2, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(layers.Activation(\"tanh\"))\n",
    "    # model.summary()\n",
    "\n",
    "    # model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "    # model.compile(Adam(learning_rate=0.0002, beta_1=0.5), loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "# Discriminator Model\n",
    "def build_discriminator(input_shape):\n",
    "    \"\"\"\n",
    "    Builds the discriminator model for a GAN.\n",
    "    \n",
    "    Parameters:\n",
    "    input_shape (tuple): The shape of the input images (height, width, channels).\n",
    "    \n",
    "    Returns:\n",
    "    A compiled Keras Sequential model.\n",
    "    \"\"\"\n",
    "    model = models.Sequential(name=\"Discriminator\")\n",
    "    print(\"Building Discriminator Model\")\n",
    "    \n",
    "    # Input layer\n",
    "    print(f\"Input shape: {input_shape}\")\n",
    "    \n",
    "    model.add(layers.Input(shape=input_shape))\n",
    "    model.add(layers.Conv2D(32, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    model.add(layers.Conv2D(32, kernel_size=4, strides=2, padding=\"same\", input_shape=input_shape))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    # Flatten and output layer\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    # model.summary()\n",
    "    \n",
    "    # model.compile(Adam(learning_rate=0.0002, beta_1=0.5), loss='binary_crossentropy')\n",
    "    # model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "    return model\n",
    "\n",
    "# GAN Model\n",
    "def compile_gan(generator, discriminator):\n",
    "    \"\"\"\n",
    "    Compiles the GAN model by connecting the generator and discriminator.\n",
    "    \n",
    "    Parameters:\n",
    "    generator (keras.Model): The generator model.\n",
    "    discriminator (keras.Model): The discriminator model.\n",
    "    \n",
    "    Returns:\n",
    "    A compiled GAN model.\n",
    "    \"\"\"\n",
    "    # Ensure the discriminator weights are not updated during generator training\n",
    "    discriminator.trainable = False\n",
    "    gan_input = layers.Input(shape=(generator.input_shape[1],))\n",
    "    generated_image = generator(gan_input)\n",
    "    gan_output = discriminator(generated_image)\n",
    "    gan = models.Model(gan_input, gan_output, name=\"GAN\")\n",
    "    gan.compile(Adam(learning_rate=0.0002, beta_1=0.5), loss='binary_crossentropy')\n",
    "    \n",
    "    return gan\n",
    "\n",
    "# Instantiate models\n",
    "latent_dim = 100\n",
    "img_shape = (200, 160, 1)\n",
    "generator = build_generator(latent_dim=latent_dim, output_shape=img_shape)\n",
    "discriminator = build_discriminator(input_shape=img_shape)\n",
    "\n",
    "generator.compile(Adam(learning_rate=0.0002, beta_1=0.5), loss='binary_crossentropy')\n",
    "discriminator.compile(Adam(learning_rate=0.0002, beta_1=0.5), loss='binary_crossentropy')\n",
    "\n",
    "gan = compile_gan(generator, discriminator)\n",
    "\n",
    "# Display model summaries\n",
    "generator.summary()\n",
    "discriminator.summary()\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "693a62fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras implmentation\n",
    "# def train_gan(generator, discriminator, gan, latent_dim, epochs, batch_size):\n",
    "#     \"\"\"\n",
    "#     Train the GAN model using images loaded from S3.\n",
    "    \n",
    "#     Args:\n",
    "#         generator: The generator model\n",
    "#         discriminator: The discriminator model\n",
    "#         gan: The combined GAN model\n",
    "#         latent_dim: Dimension of the latent space\n",
    "#         epochs: Number of training epochs\n",
    "#         batch_size: Size of training batches\n",
    "#     \"\"\"\n",
    "#     # Load images from S3\n",
    "#     s3 = boto3.resource('s3')   \n",
    "#     bucket = s3.Bucket('chemocraft-data')\n",
    "#     images = load_images_from_s3(bucket, 'Akshay/brain_slices/320/')\n",
    "    \n",
    "#     half_batch = batch_size // 2\n",
    "    \n",
    "#     # Lists to store loss values for plotting\n",
    "#     d_losses = []\n",
    "#     g_losses = []\n",
    "    \n",
    "#     # Create a directory for saving generated images\n",
    "#     os.makedirs('generated_images', exist_ok=True)\n",
    "    \n",
    "#     print(\"Starting GAN training...\")\n",
    "#     for epoch in range(epochs):\n",
    "#         # ---------------------\n",
    "#         #  Train Discriminator\n",
    "#         # ---------------------\n",
    "        \n",
    "#         # Select a random half batch of real images\n",
    "#         idx = np.random.randint(0, images.shape[0], half_batch)\n",
    "#         real_imgs = images[idx]\n",
    "        \n",
    "#         # Generate a half batch of fake images\n",
    "#         noise = np.random.normal(0, 1, (half_batch, latent_dim))\n",
    "#         gen_imgs = generator.predict(noise)\n",
    "        \n",
    "#         # Train the discriminator\n",
    "#         d_loss_real = discriminator.train_on_batch(real_imgs, np.ones((half_batch, 1)))\n",
    "#         d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "#         d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "#         # ---------------------\n",
    "#         #  Train Generator\n",
    "#         # ---------------------\n",
    "        \n",
    "#         # Generate a batch of noise samples\n",
    "#         noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        \n",
    "#         # Train the generator\n",
    "#         # The generator wants the discriminator to label the generated samples as valid (ones)\n",
    "#         g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "        \n",
    "#         # Store losses\n",
    "#         d_losses.append(d_loss[0])\n",
    "#         g_losses.append(g_loss)\n",
    "        \n",
    "#         # Print progress\n",
    "#         if epoch % 100 == 0:\n",
    "#             print(f\"Epoch {epoch}/{epochs}\")\n",
    "#             print(f\"D loss: {d_loss[0]:.4f}, acc.: {d_loss[1]:.2%}\")\n",
    "#             print(f\"G loss: {g_loss:.4f}\")\n",
    "            \n",
    "#             # Save generated images\n",
    "#             save_generated_images(epoch, generator, latent_dim)\n",
    "            \n",
    "#             # Save models periodically\n",
    "#             if epoch % 1000 == 0:\n",
    "#                 save_models(epoch, generator, discriminator)\n",
    "                \n",
    "#     return d_losses, g_losses\n",
    "\n",
    "# def save_generated_images(epoch, generator, latent_dim, examples=16, dim=(4, 4)):\n",
    "#     \"\"\"\n",
    "#     Generate and save sample images during training.\n",
    "#     \"\"\"\n",
    "#     noise = np.random.normal(0, 1, (examples, latent_dim))\n",
    "#     gen_imgs = generator.predict(noise)\n",
    "    \n",
    "#     # Rescale images from [-1, 1] to [0, 1]\n",
    "#     gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "    \n",
    "#     # Set up the plot\n",
    "#     fig = plt.figure(figsize=(8, 8))\n",
    "#     for i in range(examples):\n",
    "#         plt.subplot(dim[0], dim[1], i+1)\n",
    "#         plt.imshow(gen_imgs[i, :, :, 0], cmap='gray')\n",
    "#         plt.axis('off')\n",
    "    \n",
    "#     # Save the plot\n",
    "#     plt.savefig(f'generated_images/brain_mri_epoch_{epoch}.png')\n",
    "#     plt.close()\n",
    "\n",
    "# def save_models(epoch, generator, discriminator):\n",
    "#     \"\"\"\n",
    "#     Save the generator and discriminator models.\n",
    "#     \"\"\"\n",
    "#     generator.save(f'models/generator_epoch_{epoch}.h5')\n",
    "#     discriminator.save(f'models/discriminator_epoch_{epoch}.h5')\n",
    "\n",
    "# def plot_training_history(d_losses, g_losses):\n",
    "#     \"\"\"\n",
    "#     Plot the training history of the GAN.\n",
    "#     \"\"\"\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.plot(d_losses, label='Discriminator Loss')\n",
    "#     plt.plot(g_losses, label='Generator Loss')\n",
    "#     plt.title('GAN Training History')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     plt.savefig('training_history.png')\n",
    "#     plt.close()\n",
    "\n",
    "# # Training\n",
    "# epochs = 50\n",
    "# batch_size = 155*5\n",
    "# half_batch = batch_size // 2\n",
    "\n",
    "\n",
    "# # Train the GAN\n",
    "# d_losses, g_losses = train_gan(\n",
    "#     generator=generator,\n",
    "#     discriminator=discriminator,\n",
    "#     gan=gan,\n",
    "#     latent_dim=latent_dim,\n",
    "#     epochs=epochs,\n",
    "#     batch_size=half_batch\n",
    "# )\n",
    "    \n",
    "#     # Plot training history\n",
    "# plot_training_history(d_losses, g_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddcb97be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_gan(generator, discriminator, gan, latent_dim, epochs, batch_size):\n",
    "#     # Load images from S3\n",
    "#     s3 = boto3.resource('s3')   \n",
    "#     bucket = s3.Bucket('chemocraft-data')\n",
    "#     images = load_images_from_s3(bucket, 'Akshay/brain_slices/320/')\n",
    "\n",
    "#     half_batch = 64//2\n",
    "    \n",
    "#     print(\"Training GAN...\")\n",
    "#     for epoch in range(epochs):\n",
    "#         # Train discriminator with real images\n",
    "#         idx = np.random.randint(0, images.shape[0], half_batch)\n",
    "#         real_images = images[idx]\n",
    "#         real_labels = np.ones((half_batch, 1))\n",
    "        \n",
    "#         # Generate fake images\n",
    "#         noise = np.random.normal(0, 1, (half_batch, latent_dim))\n",
    "#         fake_images = generator.predict(noise)\n",
    "#         fake_labels = np.zeros((half_batch, 1))\n",
    "        \n",
    "#         # Train discriminator\n",
    "#         d_loss_real = discriminator.train_on_batch(real_images, real_labels)\n",
    "#         d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n",
    "#         d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
    "        \n",
    "#         # Update discriminator loss tracker\n",
    "#         if hasattr(discriminator, 'metrics') and discriminator.metrics:\n",
    "#             for metric in discriminator.metrics:\n",
    "#                 if hasattr(metric, 'update_state'):\n",
    "#                     metric.update_state(d_loss)\n",
    "        \n",
    "#         # Train generator\n",
    "#         noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "#         misleading_labels = np.ones((batch_size, 1))\n",
    "#         g_loss = gan.train_on_batch(noise, misleading_labels)\n",
    "        \n",
    "#         # Update generator loss tracker\n",
    "#         if hasattr(gan, 'metrics') and gan.metrics:\n",
    "#             for metric in gan.metrics:\n",
    "#                 if hasattr(metric, 'update_state'):\n",
    "#                     metric.update_state(g_loss)\n",
    "        \n",
    "#         # Print loss values\n",
    "#         print(f\"{epoch+1}/{epochs}, D Loss: {d_loss}, G Loss: {g_loss}\")\n",
    "        \n",
    "# train_gan(generator, discriminator, gan, latent_dim, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72e765ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_3d_image(generator, latent_dim, num_slices=150):\n",
    "#     noise = np.random.normal(0, 1, (num_slices, latent_dim))\n",
    "#     generated_slices = generator.predict(noise)\n",
    "#     generated_3d_image = np.stack(generated_slices, axis=0)  # Shape: (num_slices, 200, 160)\n",
    "#     return generated_3d_image\n",
    "\n",
    "# # generated_3d_image = generate_3d_image(generator, latent_dim=100, num_slices=150)\n",
    "\n",
    "# # Display the generated 3D image\n",
    "# # fig, axs = plt.subplots(10, 15, figsize=(15, 10))\n",
    "# # cnt = 0\n",
    "# # for i in range(10):\n",
    "# #     for j in range(15):\n",
    "# #         axs[i, j].imshow(generated_3d_image[cnt, :, :, 0], cmap='gray')\n",
    "# #         axs[i, j].axis('off')\n",
    "# #         cnt += 1\n",
    "# # plt.show()\n",
    "\n",
    "# # Save the models\n",
    "# generator.save('generator_model.h5')\n",
    "# discriminator.save('discriminator_model.h5')\n",
    "# gan.save('gan_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fa8e63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch implementation\n",
    "import io\n",
    "import numpy as np\n",
    "import boto3\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Initialize S3 resource and specify bucket and folder details\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'chemocraft-data'\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "folder_prefix = \"Akshay/brain_slices/\"\n",
    "\n",
    "# Set device for CUDA\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Function to load images from S3\n",
    "def load_images_from_s3(bucket, folder_prefix):\n",
    "    print(f\"Loading images from S3 bucket: {bucket_name}/{folder_prefix}\")\n",
    "    images = []\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(),\n",
    "        transforms.Resize((200, 160)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    for obj in bucket.objects.filter(Prefix=folder_prefix):\n",
    "        if obj.key.endswith('.png'):\n",
    "            file_stream = io.BytesIO(obj.get()['Body'].read())\n",
    "            image = Image.open(file_stream)\n",
    "            image = transform(image)  # Normalize to [0, 1]\n",
    "            images.append(image)\n",
    "    return torch.stack(images).to(device)\n",
    "\n",
    "# Generator Model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=100, output_shape=(200, 160, 1)):\n",
    "        super(Generator, self).__init__()\n",
    "        self.init_dim = (128, 25, 20)\n",
    "        self.fc = nn.Linear(latent_dim, 128 * 25 * 20)\n",
    "        \n",
    "        self.deconv_blocks = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.ConvTranspose2d(8, output_shape[2], kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.fc(z).view(-1, *self.init_dim)\n",
    "        x = self.deconv_blocks(x)\n",
    "        return x\n",
    "\n",
    "# Discriminator Model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape=(200, 160, 1)):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[2], 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.fc = nn.Linear(128 * 25 * 20, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.conv_blocks(img)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# Instantiate models and move them to CUDA\n",
    "latent_dim = 100\n",
    "img_shape = (200, 160, 1)\n",
    "generator = Generator(latent_dim=latent_dim, output_shape=img_shape).to(device)\n",
    "discriminator = Discriminator(input_shape=img_shape).to(device)\n",
    "\n",
    "# Optimizers\n",
    "lr = 0.0002\n",
    "b1 = 0.5\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(b1, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, 0.999))\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# GAN training function\n",
    "def train_gan(data_loader, num_epochs=1000):\n",
    "    for epoch in range(num_epochs):\n",
    "        for imgs in data_loader:\n",
    "            # Configure input\n",
    "            real_imgs = imgs.to(device)\n",
    "            batch_size = real_imgs.size(0)\n",
    "\n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            # Real images\n",
    "            valid = torch.ones((batch_size, 1), requires_grad=False).to(device)\n",
    "            real_loss = criterion(discriminator(real_imgs), valid)\n",
    "\n",
    "            # Fake images\n",
    "            z = torch.randn(batch_size, latent_dim, device=device)\n",
    "            fake_imgs = generator(z)\n",
    "            fake = torch.zeros((batch_size, 1), requires_grad=False).to(device)\n",
    "            fake_loss = criterion(discriminator(fake_imgs.detach()), fake)\n",
    "\n",
    "            # Total Discriminator loss\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_G.zero_grad()\n",
    "            g_loss = criterion(discriminator(fake_imgs), valid)\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}] | D loss: {d_loss.item()} | G loss: {g_loss.item()}\")\n",
    "\n",
    "# Load data from S3 and start training\n",
    "# Assuming data loader is created with loaded images and batch size\n",
    "# data_loader = DataLoader(load_images_from_s3(bucket, folder_prefix), batch_size=32, shuffle=True)\n",
    "# train_gan(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad8bed47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images from S3 bucket: chemocraft-data/Akshay/brain_slices/320/\n",
      "Starting GAN training...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x16640 and 32768x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 183\u001b[0m\n\u001b[0;32m    180\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(TensorDataset(images), batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;66;03m# Train the GAN\u001b[39;00m\n\u001b[1;32m--> 183\u001b[0m d_losses, g_losses \u001b[38;5;241m=\u001b[39m train_gan(generator, discriminator, latent_dim, epochs, batch_size, images)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# Plot training history\u001b[39;00m\n\u001b[0;32m    186\u001b[0m plot_training_history(d_losses, g_losses)\n",
      "Cell \u001b[1;32mIn[12], line 106\u001b[0m, in \u001b[0;36mtrain_gan\u001b[1;34m(generator, discriminator, latent_dim, epochs, batch_size, images)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Train with real images\u001b[39;00m\n\u001b[0;32m    105\u001b[0m optimizer_D\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 106\u001b[0m real_loss \u001b[38;5;241m=\u001b[39m criterion(discriminator(real_imgs), valid)\n\u001b[0;32m    107\u001b[0m real_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Train with fake images\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\grnpr\\anaconda3\\envs\\chemocraft\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\grnpr\\anaconda3\\envs\\chemocraft\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[12], line 59\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     57\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(img)\n\u001b[0;32m     58\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 59\u001b[0m validity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madv_layer(out)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m validity\n",
      "File \u001b[1;32mc:\\Users\\grnpr\\anaconda3\\envs\\chemocraft\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\grnpr\\anaconda3\\envs\\chemocraft\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\grnpr\\anaconda3\\envs\\chemocraft\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\grnpr\\anaconda3\\envs\\chemocraft\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\grnpr\\anaconda3\\envs\\chemocraft\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\grnpr\\anaconda3\\envs\\chemocraft\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x16640 and 32768x1)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.init_size = 256 // 4\n",
    "        self.l1 = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 1, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(1, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        ds_size = 256 // 2 ** 4\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "        return validity\n",
    "\n",
    "latent_dim = 100\n",
    "generator = Generator(latent_dim).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "def train_gan(generator, discriminator, latent_dim, epochs, batch_size, images):\n",
    "    \"\"\"\n",
    "    Train the GAN model using images loaded from S3.\n",
    "    \n",
    "    Args:\n",
    "        generator: The generator model\n",
    "        discriminator: The discriminator model\n",
    "        latent_dim: Dimension of the latent space\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Size of training batches\n",
    "        images: Tensor of real images loaded from S3\n",
    "    \"\"\"\n",
    "    # Optimizers\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Lists to store loss values for plotting\n",
    "    d_losses = []\n",
    "    g_losses = []\n",
    "\n",
    "    # Create a directory for saving generated images\n",
    "    os.makedirs('generated_images', exist_ok=True)\n",
    "\n",
    "    print(\"Starting GAN training...\")\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, len(images), batch_size):\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            real_imgs = images[i:i+batch_size].to(device)\n",
    "            batch_size = real_imgs.size(0)\n",
    "            \n",
    "            # Labels for real and fake images\n",
    "            valid = torch.ones((batch_size, 1), device=device)\n",
    "            fake = torch.zeros((batch_size, 1), device=device)\n",
    "\n",
    "            # Train with real images\n",
    "            optimizer_D.zero_grad()\n",
    "            real_loss = criterion(discriminator(real_imgs), valid)\n",
    "            real_loss.backward()\n",
    "\n",
    "            # Train with fake images\n",
    "            z = torch.randn(batch_size, latent_dim, device=device)\n",
    "            gen_imgs = generator(z)\n",
    "            fake_loss = criterion(discriminator(gen_imgs.detach()), fake)\n",
    "            fake_loss.backward()\n",
    "            \n",
    "            d_loss = real_loss + fake_loss\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "            optimizer_G.zero_grad()\n",
    "            g_loss = criterion(discriminator(gen_imgs), valid)  # We want discriminator to believe the fake images are real\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # Track losses\n",
    "            d_losses.append(d_loss.item())\n",
    "            g_losses.append(g_loss.item())\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs} | D loss: {d_loss.item():.4f} | G loss: {g_loss.item():.4f}\")\n",
    "            save_generated_images(epoch, generator, latent_dim)\n",
    "\n",
    "            # Save models periodically\n",
    "            if epoch % 100 == 0:\n",
    "                save_models(epoch, generator, discriminator)\n",
    "\n",
    "    return d_losses, g_losses\n",
    "\n",
    "def save_generated_images(epoch, generator, latent_dim, examples=16, dim=(4, 4)):\n",
    "    \"\"\"\n",
    "    Generate and save sample images during training.\n",
    "    \"\"\"\n",
    "    z = torch.randn(examples, latent_dim, device=device)\n",
    "    gen_imgs = generator(z).detach().cpu()\n",
    "    gen_imgs = 0.5 * (gen_imgs + 1)  # Rescale images from [-1, 1] to [0, 1]\n",
    "    \n",
    "    save_image(gen_imgs, f'generated_images/brain_mri_epoch_{epoch}.png', nrow=dim[0], normalize=True)\n",
    "\n",
    "def save_models(epoch, generator, discriminator):\n",
    "    \"\"\"\n",
    "    Save the generator and discriminator models.\n",
    "    \"\"\"\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    torch.save(generator.state_dict(), f'models/generator_epoch_{epoch}.pth')\n",
    "    torch.save(discriminator.state_dict(), f'models/discriminator_epoch_{epoch}.pth')\n",
    "\n",
    "def plot_training_history(d_losses, g_losses):\n",
    "    \"\"\"\n",
    "    Plot the training history of the GAN.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(d_losses, label='Discriminator Loss')\n",
    "    plt.plot(g_losses, label='Generator Loss')\n",
    "    plt.title('GAN Training History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.close()\n",
    "\n",
    "# Training parameters\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "# Load images from S3 bucket and create a DataLoader\n",
    "images = load_images_from_s3(bucket, 'Akshay/brain_slices/320/')\n",
    "train_loader = DataLoader(TensorDataset(images), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Train the GAN\n",
    "d_losses, g_losses = train_gan(generator, discriminator, latent_dim, epochs, batch_size, images)\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(d_losses, g_losses)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemocraft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
