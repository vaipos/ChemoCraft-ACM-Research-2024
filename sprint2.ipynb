{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de8e0dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (1.35.44)\n",
      "Requirement already satisfied: nibabel in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (5.3.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: torch in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (2.5.1+cu124)\n",
      "Requirement already satisfied: torchvision in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (0.20.1+cu124)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (2.5.1+cu124)\n",
      "Requirement already satisfied: pillow in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (11.0.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: torchsummary in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.44 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from boto3) (1.35.44)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from boto3) (0.10.3)\n",
      "Requirement already satisfied: packaging>=20 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from nibabel) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from nibabel) (4.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: filelock in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from torch) (75.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (5.28.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.67.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from botocore<1.36.0,>=1.35.44->boto3) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.1.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\grnpr\\anaconda3\\envs\\chemocraft\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install the required packages for the notebook\n",
    "%pip install boto3 nibabel numpy matplotlib torch torchvision torchaudio pillow tensorflow torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1950f2bf-d1fd-4c35-b6b7-8318c816c6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprint 1: Reading NII files from S3 and saving PNGs\n",
    "import io\n",
    "import os\n",
    "import boto3\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tempfile\n",
    "\n",
    "# Initialize S3 resource and specify bucket and folder details\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'chemocraft-data'\n",
    "folder_path = 'MICCAI_BraTS2020_TrainingData/'\n",
    "# folder_path = 'Data/BraTS20_Training_369 copy/'\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "def plot_slice(data, crop, slice_idx, filename):\n",
    "    # Crop the specified slice\n",
    "    slice_2d = data[:, :, slice_idx]\n",
    "    cropped_slice = slice_2d[crop[0][0]:crop[0][1], crop[1][0]:crop[1][1]]\n",
    "    \n",
    "    # Display the cropped slice with matplotlib\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(cropped_slice, cmap='gray')\n",
    "    plt.title(f'Slice {slice_idx} of {filename}')\n",
    "    plt.axis('off')  # Hide axes for cleaner display\n",
    "    plt.show()\n",
    "\n",
    "def savePNG(data, crop, filename):    \n",
    "    # Prepare directory structure\n",
    "    fileWOext = filename.split(\".\")[0]\n",
    "    TrainingCount = fileWOext.split(\"_\")[-2]\n",
    "    ScanType = fileWOext.split(\"_\")[-1]\n",
    "    slice_path = f\"brain_slices/{TrainingCount}/{ScanType}/\"\n",
    "    print(f\"Saving in directory: {slice_path}\")\n",
    "\n",
    "    # Iterate through each slice in the Z-Dimiension data and save as PNG\n",
    "    for slice_idx in range(data.shape[2]):\n",
    "        # Crop each slice\n",
    "        slice_2d = data[:, :, slice_idx]\n",
    "        cropped_slice = slice_2d[crop[0][0]:crop[0][1], crop[1][0]:crop[1][1]]\n",
    "        png_filename = f\"{slice_path}{slice_idx}.png\"\n",
    "        \n",
    "        # Local Saving\n",
    "        # try:\n",
    "        #     # Create directories as needed and save each slice\n",
    "        #     os.makedirs(slice_path, exist_ok=True)\n",
    "        #     mpimg.imsave(png_filename, cropped_slice, cmap='gray')\n",
    "        # except Exception as e:\n",
    "        #     print(f\"ERROR: directory could not be made due to {e}\")\n",
    "        \n",
    "        # Upload each PNG to S3\n",
    "        with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as temp_png:\n",
    "            mpimg.imsave(temp_png.name, cropped_slice, cmap='gray')\n",
    "            temp_png.flush()\n",
    "            temp_png.seek(0)\n",
    "            temp_png_name = temp_png.name  # Store the name to use it after the file is closed\n",
    "\n",
    "        try:\n",
    "            s3.Bucket(bucket_name).upload_file(temp_png_name, f\"Akshay/{png_filename}\")\n",
    "            os.remove(temp_png_name)\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Could not upload or delete temporary PNG file due to {e}\")\n",
    "\n",
    "def render_nii_from_s3(filename, path):\n",
    "    print(f\"Fetching file: {filename}\")\n",
    "\n",
    "    try:\n",
    "        obj = bucket.Object(path + filename)\n",
    "        file_stream = io.BytesIO(obj.get()['Body'].read())\n",
    "    except s3.meta.client.exceptions.NoSuchKey as e:\n",
    "        print(f\"ERROR: The specified key does not exist: {path + filename}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: An unexpected error occurred: {e}\")\n",
    "        return\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(suffix='.nii', delete=False) as temp_file:  # Prevent auto-deletion\n",
    "        temp_file.write(file_stream.getvalue())\n",
    "        temp_file.flush()\n",
    "\n",
    "        temp_file_path = temp_file.name\n",
    "        print(f\"Temporary file created: {temp_file_path}\")\n",
    "\n",
    "    try:\n",
    "        img = nib.load(temp_file_path)\n",
    "        data = img.get_fdata()\n",
    "\n",
    "        print(f\"Data shape for {filename}: {data.shape}\")\n",
    "        \n",
    "        if data.size == 0:\n",
    "            print(f\"No data found in {filename}\")\n",
    "            return\n",
    "\n",
    "        # Define crop dimensions\n",
    "        cropleft = 25\n",
    "        cropright = data.shape[0] - 15\n",
    "        cropbottom = data.shape[1] - 40\n",
    "        croptop = 40\n",
    "        \n",
    "        crop = np.array([[croptop, cropbottom], [cropleft, cropright]])\n",
    "        \n",
    "        # Save the PNGs and plot a sample slice\n",
    "        savePNG(data, crop, filename)\n",
    "        \n",
    "        # slice_idx = 88  # Choose a slice index for sample display\n",
    "        # plot_slice(data, crop, slice_idx, filename)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file {filename}: {e}\")\n",
    "        \n",
    "    finally:\n",
    "        try:\n",
    "            os.remove(temp_file_path)\n",
    "            print(f\"Deleted temporary file: {temp_file_path}\")\n",
    "        except OSError as cleanup_error:\n",
    "            print(f\"Error deleting temp file: {cleanup_error}\")\n",
    "\n",
    "def find_and_render_nii_files():\n",
    "    found_files = False\n",
    "\n",
    "    subfolders = set()  # use a set to ensure unique subfolder names\n",
    "    for obj in bucket.objects.filter(Prefix=folder_path):\n",
    "        # Get the path after the 'Data/' prefix and split it by '/'\n",
    "        path_parts = obj.key[len(folder_path):].split('/')\n",
    "        \n",
    "        # Check if there's at least one part (indicating a subfolder)\n",
    "        if len(path_parts) > 1:\n",
    "            subfolders.add(f'{path_parts[0]}/')  # Add the subfolder name\n",
    "            \n",
    "    subfolders = sorted(subfolders)\n",
    "\n",
    "    print(f\"Root Directory: {folder_path.split('/')[0]}\")\n",
    "    # print(subfolders)\n",
    "\n",
    "    for subfolder in subfolders:\n",
    "        path = folder_path + subfolder\n",
    "        print(f\"Reading S3 in {path}\")\n",
    "        for obj in bucket.objects.filter(Prefix=path):\n",
    "            if obj.key.endswith('.nii'):\n",
    "                print(f\"path: {path}\")\n",
    "                found_files = True\n",
    "                filename = obj.key.split('/')[-1]  # Extract filename from path\n",
    "                print(f\"Found .nii file: {filename}\")\n",
    "                render_nii_from_s3(filename, path)\n",
    "\n",
    "    if not found_files:\n",
    "        print(f\"No .nii files found in the folder {folder_path}\")\n",
    "\n",
    "# Main function\n",
    "# find_and_render_nii_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fa8e63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Device Available: True\n",
      "CUDA Device Count: 1\n",
      "Current CUDA Device: 0\n",
      "CUDA Device Name: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "# Sprint 2: GAN for Brain MRI Generation\n",
    "# pytorch implementation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torchsummary import summary\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Initialize S3 resource and specify bucket and folder details\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'chemocraft-data'\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "folder_prefix = \"Akshay/brain_slices/\"\n",
    "\n",
    "\n",
    "# Set device for CUDA\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Print CUDA device information\n",
    "print(f\"CUDA Device Available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA Device Count: {torch.cuda.device_count()}\")\n",
    "print(f\"Current CUDA Device: {torch.cuda.current_device()}\")\n",
    "print(f\"CUDA Device Name: {torch.cuda.get_device_name(device)}\")\n",
    "\n",
    "# Function to load images from S3\n",
    "# Load data from S3 and start training\n",
    "def load_images_from_s3(bucket, folder_prefix):\n",
    "    print(f\"Loading images from S3 bucket: {bucket_name}/{folder_prefix}\")\n",
    "    images = []\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(),\n",
    "        transforms.Resize((200, 160)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    for obj in bucket.objects.filter(Prefix=folder_prefix):\n",
    "        if obj.key.endswith('.png'):\n",
    "            file_stream = io.BytesIO(obj.get()['Body'].read())\n",
    "            image = Image.open(file_stream)\n",
    "            image = transform(image)  # Normalize to [0, 1]\n",
    "            images.append(image)\n",
    "    return torch.stack(images).to(device)\n",
    "\n",
    "def save_generated_images(epoch, generator, latent_dim, examples=16, dim=(4, 4)):\n",
    "    \"\"\"\n",
    "    Generate and save sample images during training.\n",
    "    \"\"\"\n",
    "    z = torch.randn(examples, latent_dim, device=device)\n",
    "    gen_imgs = generator(z).detach().cpu()\n",
    "    gen_imgs = 0.5 * (gen_imgs + 1)  # Rescale images from [-1, 1] to [0, 1]\n",
    "    \n",
    "    save_image(gen_imgs, f'generated_images/brain_mri_epoch_{epoch}.png', nrow=dim[0], normalize=True)\n",
    "\n",
    "def save_models(epoch, generator, discriminator):\n",
    "    \"\"\"\n",
    "    Save the generator and discriminator models.\n",
    "    \"\"\"\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    torch.save(generator.state_dict(), f'models/generator_epoch_{epoch}.pth')\n",
    "    torch.save(discriminator.state_dict(), f'models/discriminator_epoch_{epoch}.pth')\n",
    "\n",
    "def plot_training_history(d_losses, g_losses):\n",
    "    \"\"\"\n",
    "    Plot the training history of the GAN.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(d_losses, label='Discriminator Loss')\n",
    "    plt.plot(g_losses, label='Generator Loss')\n",
    "    plt.title('GAN Training History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72e765ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_3d_image(generator, latent_dim, num_slices=150):\n",
    "#     noise = np.random.normal(0, 1, (num_slices, latent_dim))\n",
    "#     generated_slices = generator.predict(noise)\n",
    "#     generated_3d_image = np.stack(generated_slices, axis=0)  # Shape: (num_slices, 200, 160)\n",
    "#     return generated_3d_image\n",
    "\n",
    "# # generated_3d_image = generate_3d_image(generator, latent_dim=100, num_slices=150)\n",
    "\n",
    "# # Display the generated 3D image\n",
    "# # fig, axs = plt.subplots(10, 15, figsize=(15, 10))\n",
    "# # cnt = 0\n",
    "# # for i in range(10):\n",
    "# #     for j in range(15):\n",
    "# #         axs[i, j].imshow(generated_3d_image[cnt, :, :, 0], cmap='gray')\n",
    "# #         axs[i, j].axis('off')\n",
    "# #         cnt += 1\n",
    "# # plt.show()\n",
    "\n",
    "# # Save the models\n",
    "# generator.save('generator_model.h5')\n",
    "# discriminator.save('discriminator_model.h5')\n",
    "# gan.save('gan_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad8bed47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generator Summary:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                [-1, 64000]       6,464,000\n",
      "              ReLU-2                [-1, 64000]               0\n",
      "       BatchNorm2d-3          [-1, 128, 25, 20]             256\n",
      "          Upsample-4          [-1, 128, 50, 40]               0\n",
      "            Conv2d-5          [-1, 128, 50, 40]         147,584\n",
      "       BatchNorm2d-6          [-1, 128, 50, 40]             256\n",
      "         LeakyReLU-7          [-1, 128, 50, 40]               0\n",
      "          Upsample-8         [-1, 128, 100, 80]               0\n",
      "            Conv2d-9          [-1, 64, 100, 80]          73,792\n",
      "      BatchNorm2d-10          [-1, 64, 100, 80]             128\n",
      "        LeakyReLU-11          [-1, 64, 100, 80]               0\n",
      "         Upsample-12         [-1, 64, 200, 160]               0\n",
      "           Conv2d-13          [-1, 1, 200, 160]             577\n",
      "             Tanh-14          [-1, 1, 200, 160]               0\n",
      "================================================================\n",
      "Total params: 6,686,593\n",
      "Trainable params: 6,686,593\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 44.92\n",
      "Params size (MB): 25.51\n",
      "Estimated Total Size (MB): 70.43\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Discriminator Summary:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 16, 100, 80]             160\n",
      "         LeakyReLU-2          [-1, 16, 100, 80]               0\n",
      "         Dropout2d-3          [-1, 16, 100, 80]               0\n",
      "            Conv2d-4           [-1, 32, 50, 40]           4,640\n",
      "         LeakyReLU-5           [-1, 32, 50, 40]               0\n",
      "         Dropout2d-6           [-1, 32, 50, 40]               0\n",
      "       BatchNorm2d-7           [-1, 32, 50, 40]              64\n",
      "            Conv2d-8           [-1, 64, 25, 20]          18,496\n",
      "         LeakyReLU-9           [-1, 64, 25, 20]               0\n",
      "        Dropout2d-10           [-1, 64, 25, 20]               0\n",
      "      BatchNorm2d-11           [-1, 64, 25, 20]             128\n",
      "           Conv2d-12          [-1, 128, 13, 10]          73,856\n",
      "        LeakyReLU-13          [-1, 128, 13, 10]               0\n",
      "        Dropout2d-14          [-1, 128, 13, 10]               0\n",
      "      BatchNorm2d-15          [-1, 128, 13, 10]             256\n",
      "           Linear-16                    [-1, 1]          16,641\n",
      "          Sigmoid-17                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 114,241\n",
      "Trainable params: 114,241\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.12\n",
      "Forward/backward pass size (MB): 6.37\n",
      "Params size (MB): 0.44\n",
      "Estimated Total Size (MB): 6.93\n",
      "----------------------------------------------------------------\n",
      "1. Input noise shape: torch.Size([32, 100])\n",
      "2. Generated images shape: torch.Size([32, 1, 200, 160])\n",
      "3. After discriminator conv layers: torch.Size([32, 128, 13, 10])\n",
      "4. After flattening: torch.Size([32, 16640])\n",
      "5. Final output shape: torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        # We'll start with 25x20 (200/8 x 160/8) and upsample 3 times to get 200x160\n",
    "        self.init_height = 25  # 200 // 8\n",
    "        self.init_width = 20   # 160 // 8\n",
    "        \n",
    "        # Initial layer to convert latent vector to features\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128 * self.init_height * self.init_width),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),  # 50x40\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),  # 100x80\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),  # 200x160\n",
    "            nn.Conv2d(64, 1, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(-1, 128, self.init_height, self.init_width)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), \n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(1, 16, bn=False),    # Output: 100x80\n",
    "            *discriminator_block(16, 32),             # Output: 50x40\n",
    "            *discriminator_block(32, 64),             # Output: 25x20\n",
    "            *discriminator_block(64, 128),            # Output: 13x10\n",
    "        )\n",
    "\n",
    "        # Calculate the correct input size for the final linear layer\n",
    "        # After 4 stride-2 convolutions: 200x160 -> 100x80 -> 50x40 -> 25x20 -> 13x10\n",
    "        ds_height = int(np.ceil(200 / 2**4))  # = 13\n",
    "        ds_width = int(np.ceil(160 / 2**4))   # = 10\n",
    "        self.adv_layer = nn.Sequential(\n",
    "            nn.Linear(128 * ds_height * ds_width, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "        return validity\n",
    "    \n",
    "def debug_dimensions():\n",
    "\n",
    "    # Test forward pass with dimension printing\n",
    "    z = torch.randn(batch_size, latent_dim, device=device)\n",
    "    print(f\"1. Input noise shape: {z.shape}\")\n",
    "\n",
    "    # Generator forward pass\n",
    "    fake_imgs = generator(z)\n",
    "    print(f\"2. Generated images shape: {fake_imgs.shape}\")\n",
    "\n",
    "    # Discriminator forward pass with detailed shape printing\n",
    "    d_out = discriminator.model(fake_imgs)\n",
    "    print(f\"3. After discriminator conv layers: {d_out.shape}\")\n",
    "    \n",
    "    d_out_flat = d_out.view(d_out.shape[0], -1)\n",
    "    print(f\"4. After flattening: {d_out_flat.shape}\")\n",
    "    \n",
    "    validity = discriminator.adv_layer(d_out_flat)\n",
    "    print(f\"5. Final output shape: {validity.shape}\")\n",
    "\n",
    "    torch.cuda.empty_cache()  # Clear the CUDA cache\n",
    "\n",
    "    # Print model summaries\n",
    "    # summary(generator, input_size=(batch_size, latent_dim))\n",
    "    \n",
    "    # summary(discriminator, input_size=(batch_size, 1, 200, 160))\n",
    "\n",
    "latent_dim = 100\n",
    "batch_size = 32  # Reduce the batch size to fit in GPU memory\n",
    "generator = Generator(latent_dim).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "torch.cuda.empty_cache()  # Clear the CUDA cache\n",
    "\n",
    "# Print model summaries\n",
    "print(\"\\nGenerator Summary:\")\n",
    "summary(generator, input_size=(latent_dim,))\n",
    "print(\"\\nDiscriminator Summary:\")\n",
    "summary(discriminator, input_size=(1, 200, 160))\n",
    "\n",
    "debug_dimensions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a181037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images from S3 bucket: chemocraft-data/Akshay/brain_slices/320\n",
      "\n",
      "Generator Summary:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                [-1, 64000]       6,464,000\n",
      "              ReLU-2                [-1, 64000]               0\n",
      "       BatchNorm2d-3          [-1, 128, 25, 20]             256\n",
      "          Upsample-4          [-1, 128, 50, 40]               0\n",
      "            Conv2d-5          [-1, 128, 50, 40]         147,584\n",
      "       BatchNorm2d-6          [-1, 128, 50, 40]             256\n",
      "         LeakyReLU-7          [-1, 128, 50, 40]               0\n",
      "          Upsample-8         [-1, 128, 100, 80]               0\n",
      "            Conv2d-9          [-1, 64, 100, 80]          73,792\n",
      "      BatchNorm2d-10          [-1, 64, 100, 80]             128\n",
      "        LeakyReLU-11          [-1, 64, 100, 80]               0\n",
      "         Upsample-12         [-1, 64, 200, 160]               0\n",
      "           Conv2d-13          [-1, 1, 200, 160]             577\n",
      "             Tanh-14          [-1, 1, 200, 160]               0\n",
      "================================================================\n",
      "Total params: 6,686,593\n",
      "Trainable params: 6,686,593\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 44.92\n",
      "Params size (MB): 25.51\n",
      "Estimated Total Size (MB): 70.43\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Discriminator Summary:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 16, 100, 80]             160\n",
      "         LeakyReLU-2          [-1, 16, 100, 80]               0\n",
      "         Dropout2d-3          [-1, 16, 100, 80]               0\n",
      "            Conv2d-4           [-1, 32, 50, 40]           4,640\n",
      "         LeakyReLU-5           [-1, 32, 50, 40]               0\n",
      "         Dropout2d-6           [-1, 32, 50, 40]               0\n",
      "       BatchNorm2d-7           [-1, 32, 50, 40]              64\n",
      "            Conv2d-8           [-1, 64, 25, 20]          18,496\n",
      "         LeakyReLU-9           [-1, 64, 25, 20]               0\n",
      "        Dropout2d-10           [-1, 64, 25, 20]               0\n",
      "      BatchNorm2d-11           [-1, 64, 25, 20]             128\n",
      "           Conv2d-12          [-1, 128, 13, 10]          73,856\n",
      "        LeakyReLU-13          [-1, 128, 13, 10]               0\n",
      "        Dropout2d-14          [-1, 128, 13, 10]               0\n",
      "      BatchNorm2d-15          [-1, 128, 13, 10]             256\n",
      "           Linear-16                    [-1, 1]          16,641\n",
      "          Sigmoid-17                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 114,241\n",
      "Trainable params: 114,241\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.12\n",
      "Forward/backward pass size (MB): 6.37\n",
      "Params size (MB): 0.44\n",
      "Estimated Total Size (MB): 6.93\n",
      "----------------------------------------------------------------\n",
      "Starting GAN training...\n",
      "Epoch 0/500 | D loss: 1.2938 | G loss: 0.7915\n",
      "Epoch 10/500 | D loss: 1.2367 | G loss: 0.5900\n",
      "Epoch 20/500 | D loss: 0.1466 | G loss: 1.8807\n",
      "Epoch 30/500 | D loss: 1.6275 | G loss: 1.4596\n",
      "Epoch 40/500 | D loss: 1.3691 | G loss: 0.9388\n",
      "Epoch 50/500 | D loss: 0.8484 | G loss: 0.9481\n",
      "Epoch 60/500 | D loss: 0.5420 | G loss: 1.5025\n",
      "Epoch 70/500 | D loss: 0.8106 | G loss: 1.8608\n",
      "Epoch 80/500 | D loss: 1.0558 | G loss: 1.2476\n",
      "Epoch 90/500 | D loss: 0.1298 | G loss: 2.3195\n",
      "Epoch 100/500 | D loss: 0.0917 | G loss: 2.0300\n",
      "Epoch 110/500 | D loss: 0.1484 | G loss: 2.0603\n",
      "Epoch 120/500 | D loss: 0.3529 | G loss: 1.4508\n",
      "Epoch 130/500 | D loss: 0.5049 | G loss: 2.4809\n",
      "Epoch 140/500 | D loss: 0.2795 | G loss: 2.9136\n",
      "Epoch 150/500 | D loss: 0.4182 | G loss: 0.8281\n",
      "Epoch 160/500 | D loss: 0.4853 | G loss: 2.0172\n",
      "Epoch 170/500 | D loss: 1.2339 | G loss: 1.4477\n",
      "Epoch 180/500 | D loss: 0.2899 | G loss: 1.9658\n",
      "Epoch 190/500 | D loss: 0.6791 | G loss: 1.5877\n",
      "Epoch 200/500 | D loss: 0.1357 | G loss: 2.0725\n",
      "Epoch 210/500 | D loss: 0.1916 | G loss: 1.9462\n",
      "Epoch 220/500 | D loss: 0.1356 | G loss: 2.6210\n",
      "Epoch 230/500 | D loss: 0.4743 | G loss: 1.4794\n",
      "Epoch 240/500 | D loss: 0.1417 | G loss: 2.3272\n",
      "Epoch 250/500 | D loss: 0.0959 | G loss: 2.2294\n",
      "Epoch 260/500 | D loss: 0.2832 | G loss: 0.6397\n",
      "Epoch 270/500 | D loss: 0.1315 | G loss: 1.9434\n",
      "Epoch 280/500 | D loss: 1.1806 | G loss: 1.2707\n",
      "Epoch 290/500 | D loss: 1.8554 | G loss: 1.8801\n",
      "Epoch 300/500 | D loss: 0.8353 | G loss: 1.7030\n",
      "Epoch 310/500 | D loss: 0.6477 | G loss: 1.6565\n",
      "Epoch 320/500 | D loss: 0.4782 | G loss: 1.5132\n",
      "Epoch 330/500 | D loss: 0.8178 | G loss: 1.6704\n",
      "Epoch 340/500 | D loss: 0.3882 | G loss: 1.4629\n",
      "Epoch 350/500 | D loss: 0.2444 | G loss: 2.2320\n",
      "Epoch 360/500 | D loss: 0.5093 | G loss: 1.8130\n",
      "Epoch 370/500 | D loss: 0.6417 | G loss: 1.4419\n",
      "Epoch 380/500 | D loss: 0.4256 | G loss: 0.8834\n",
      "Epoch 390/500 | D loss: 0.2569 | G loss: 1.7482\n",
      "Epoch 400/500 | D loss: 0.2688 | G loss: 0.9468\n",
      "Epoch 410/500 | D loss: 0.0960 | G loss: 3.0736\n",
      "Epoch 420/500 | D loss: 0.3919 | G loss: 0.5668\n",
      "Epoch 430/500 | D loss: 0.1500 | G loss: 2.0743\n",
      "Epoch 440/500 | D loss: 0.1914 | G loss: 2.3385\n",
      "Epoch 450/500 | D loss: 0.1053 | G loss: 2.1357\n",
      "Epoch 460/500 | D loss: 0.1799 | G loss: 2.5413\n",
      "Epoch 470/500 | D loss: 0.2672 | G loss: 2.1292\n",
      "Epoch 480/500 | D loss: 0.4313 | G loss: 1.4678\n",
      "Epoch 490/500 | D loss: 0.3509 | G loss: 1.1646\n"
     ]
    }
   ],
   "source": [
    "def train_gan(generator, discriminator, latent_dim, epochs, batch_size, images, accumulation_steps=8):\n",
    "    \"\"\"\n",
    "    Train the GAN model using images loaded from S3.\n",
    "    \n",
    "    Args:\n",
    "        generator: The generator model\n",
    "        discriminator: The discriminator model\n",
    "        latent_dim: Dimension of the latent space\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Size of training batches\n",
    "        images: Tensor of real images loaded from S3\n",
    "    \"\"\"\n",
    "    # Optimizers\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Lists to store loss values for plotting\n",
    "    d_losses = []\n",
    "    g_losses = []\n",
    "\n",
    "    # Create a directory for saving generated images\n",
    "    os.makedirs('generated_images', exist_ok=True)\n",
    "\n",
    "    print(\"Starting GAN training...\")\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, len(images), batch_size):\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            real_imgs = images[i:i+batch_size].to(device)\n",
    "            batch_size = real_imgs.size(0)\n",
    "            \n",
    "            # Labels for real and fake images\n",
    "            valid = torch.ones((batch_size, 1), device=device)\n",
    "            fake = torch.zeros((batch_size, 1), device=device)\n",
    "\n",
    "            # Train with real images\n",
    "            optimizer_D.zero_grad()\n",
    "            real_loss = criterion(discriminator(real_imgs), valid)\n",
    "            real_loss.backward()\n",
    "\n",
    "            # Train with fake images\n",
    "            z = torch.randn(batch_size, latent_dim, device=device)\n",
    "            gen_imgs = generator(z)\n",
    "            fake_loss = criterion(discriminator(gen_imgs.detach()), fake)\n",
    "            fake_loss.backward()\n",
    "            \n",
    "            d_loss = real_loss + fake_loss\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "            optimizer_G.zero_grad()\n",
    "            g_loss = criterion(discriminator(gen_imgs), valid)  # We want discriminator to believe the fake images are real\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "            # Accumulate gradients\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                optimizer_G.step()\n",
    "                optimizer_G.zero_grad()\n",
    "                optimizer_D.step()\n",
    "                optimizer_D.zero_grad()\n",
    "\n",
    "            # Track losses\n",
    "            d_losses.append(d_loss.item())\n",
    "            g_losses.append(g_loss.item())\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs} | D loss: {d_loss.item():.4f} | G loss: {g_loss.item():.4f}\")\n",
    "            save_generated_images(epoch, generator, latent_dim)\n",
    "\n",
    "            # Save models periodically\n",
    "            if epoch % 100 == 0:\n",
    "                save_models(epoch, generator, discriminator)\n",
    "\n",
    "    return d_losses, g_losses\n",
    "\n",
    "# Training parameters\n",
    "epochs = 500\n",
    "batch_size = 16  # Reduce the batch size to fit in GPU memory\n",
    "\n",
    "# Load images from S3 bucket and create a DataLoader\n",
    "images = load_images_from_s3(bucket, 'Akshay/brain_slices/320')\n",
    "train_loader = DataLoader(TensorDataset(images), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Train the GAN\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # Set the environment variable\n",
    "d_losses, g_losses = train_gan(generator, discriminator, latent_dim, epochs, batch_size, images)\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(d_losses, g_losses)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemocraft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
